---
sidebar_position: 1
---

# Vectors and Embeddings

Understanding vectors and embeddings is fundamental to working with Needle. This page explains what they are and how they're used in vector search.

## What is a Vector?

In Needle, a **vector** is simply an array of floating-point numbers. For example:

```rust
let vector: Vec<f32> = vec![0.1, 0.2, 0.3, -0.1, 0.5];
```

This is a 5-dimensional vector. In practice, vectors used in AI applications typically have between 128 and 4096 dimensions.

## What are Embeddings?

**Embeddings** are vectors that represent the semantic meaning of data. They're generated by machine learning models trained to map similar items to nearby points in vector space.

For example, the embeddings for "king" and "monarch" would be close together, while "king" and "banana" would be far apart.

### Common Embedding Models

| Model | Dimensions | Use Case |
|-------|------------|----------|
| all-MiniLM-L6-v2 | 384 | General-purpose text |
| text-embedding-3-small | 1536 | OpenAI text embeddings |
| text-embedding-3-large | 3072 | High-quality OpenAI embeddings |
| CLIP ViT-B/32 | 512 | Images and text |
| BGE-large-en-v1.5 | 1024 | High-quality text retrieval |
| Cohere embed-v3 | 1024 | Multilingual text |

## Generating Embeddings

Needle stores and searches vectorsâ€”it doesn't generate them. You'll need an embedding model to convert your data into vectors.

### Using OpenAI

```rust
use openai_api_rust::*;
use openai_api_rust::embeddings::*;

async fn get_embedding(text: &str) -> Vec<f32> {
    let auth = Auth::from_env().unwrap();
    let openai = OpenAI::new(auth, "https://api.openai.com/v1/");

    let body = EmbeddingsBody {
        model: "text-embedding-3-small".into(),
        input: text.into(),
        ..Default::default()
    };

    let result = openai.embeddings_create(&body).await.unwrap();
    result.data[0].embedding.clone()
}
```

### Using Sentence Transformers (Python)

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(['Hello world', 'How are you?'])
```

### Using ONNX Embeddings (Built-in)

With the `embeddings` feature, Needle can generate embeddings directly:

```rust
use needle::EmbeddingModel;

let model = EmbeddingModel::load("all-MiniLM-L6-v2")?;
let embedding = model.encode("Hello, world!")?;
```

## Vector Properties

### Dimensionality

The **dimensionality** of a vector is the number of elements it contains. All vectors in a collection must have the same dimensionality.

```rust
// Create a collection for 384-dimensional vectors
db.create_collection("docs", 384)?;

// This will work
collection.insert("doc1", &vec![0.0; 384], Some(json!({})))?;

// This will fail - wrong dimension
collection.insert("doc2", &vec![0.0; 512], Some(json!({})))?;
```

### Normalization

For **cosine similarity**, vectors should be normalized (length = 1). Needle handles this automatically when you use `DistanceFunction::Cosine`.

```rust
// Needle normalizes automatically for cosine distance
db.create_collection("docs", 384)?;

// For other distance functions, you may want to normalize manually
fn normalize(v: &mut Vec<f32>) {
    let norm: f32 = v.iter().map(|x| x * x).sum::<f32>().sqrt();
    if norm > 0.0 {
        v.iter_mut().for_each(|x| *x /= norm);
    }
}
```

## Vector Storage in Needle

Needle stores vectors efficiently using several techniques:

### Memory Mapping

For large databases (>10MB), Needle uses memory-mapped files to avoid loading all vectors into RAM:

```rust
// Needle automatically uses mmap for large files
let db = Database::open("large_database.needle")?;
```

### Quantization

Reduce memory usage with quantization:

```rust
use needle::{QuantizationType, CollectionConfig};

// Create collection with scalar quantization (4x compression)
let config = CollectionConfig::new("docs", 384)
    .with_distance(DistanceFunction::Cosine)
    .with_quantization(QuantizationType::Scalar);
db.create_collection_with_config(config)?;
```

See the [Quantization Guide](/docs/guides/quantization) for details.

## Best Practices

### Choose the Right Dimensions

- **Higher dimensions** capture more nuance but use more memory and are slower
- **384 dimensions** is a good balance for most text applications
- **1024+ dimensions** for high-stakes retrieval where quality is critical

### Batch Your Embeddings

Generating embeddings is often the bottleneck. Batch your requests:

```rust
// Batch insert for efficiency
let vectors: Vec<(&str, Vec<f32>, Value)> = documents
    .iter()
    .map(|doc| (doc.id, get_embedding(&doc.text), json!({"title": doc.title})))
    .collect();

for (id, vector, metadata) in vectors {
    collection.insert(id, &vector, Some(metadata))?;
}
```

### Use Consistent Embedding Models

Always use the same embedding model for indexing and querying. Mixing models will produce poor results:

```rust
// WRONG: Different models for indexing and search
// Index with all-MiniLM-L6-v2, search with OpenAI - BAD!

// CORRECT: Same model for both
let model = "all-MiniLM-L6-v2";
let doc_embedding = embed(model, "Document text");
let query_embedding = embed(model, "Search query");
```

## Next Steps

- [Collections](/docs/concepts/collections) - Learn about organizing vectors
- [Distance Functions](/docs/concepts/distance-functions) - Choose the right similarity metric
- [HNSW Index](/docs/concepts/hnsw-index) - Understand how search works
